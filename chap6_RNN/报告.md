# 解释 RNN ，LSTM，GRU模型

### **1. RNN**

RNN是一种用于处理序列数据的神经网络，其特殊之处在于具有“循环连接”，能够保留和利用之前的输入信息。其核心思想是将每个时间步的输出与下一个时间步的输入相结合，从而在时间维度上形成“记忆”。

**公式表示**
 设 $$x_t$$ 为当前时间步的输入，$$h_t$$ 为当前时间步的隐藏状态，$$y_t$$ 为当前时间步的输出：
$$
h_t = f(W_{hx}x_t + W_{hh}h_{t-1} + b_h)
$$
$$
y_t = g(W_{hy}h_t + b_y)
$$

其中，$$f$$ 通常是非线性激活函数（如`tanh`或`ReLU`），$$g$$ 为输出层的激活函数（如softmax）。

**缺点**

- 训练时容易出现梯度消失或梯度爆炸问题，导致模型难以捕获长距离依赖关系。

------

### **2. LSTM**

LSTM 是 RNN 的改进版本，专为解决梯度消失问题设计。它引入了门控机制，通过精细的控制来决定哪些信息应被保留、更新或遗忘。

**LSTM的核心结构**

- **遗忘门（Forget Gate）**：决定遗忘哪些信息。
- **输入门（Input Gate）**：决定更新哪些信息。
- **输出门（Output Gate）**：决定当前时间步的输出。

**公式表示**

- 遗忘门：$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
- 输入门：$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
- 新的候选记忆单元：$$\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)$$
- 当前记忆单元：$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$
- 输出门：$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
- 当前隐藏状态：$$h_t = o_t \odot \tanh(C_t)$$

**优势**
 LSTM 能更有效地捕获长距离依赖信息，适用于长序列数据。

------

### **3. GRU（Gated Recurrent Unit）**

GRU 是 LSTM 的简化版本，它将输入门和遗忘门合并成一个“更新门”，并用“重置门”控制当前信息对上一时间步的依赖程度。

**公式表示**

- 更新门：$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$
- 重置门：$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$
- 候选隐藏状态：$$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$
- 当前隐藏状态：$$h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t$$

**优势**
 GRU 结构更简单，参数更少，训练速度更快，且在很多场景下表现接近甚至优于 LSTM。

------



# 叙述一下诗歌生成的过程

#### **1. 数据准备**

包括预处理数据集，分析词频等。

------

#### **2. 模型搭建**

- 使用 **Embedding层** 将每个词转换为稠密向量表示，捕获其语义信息。
- 使用 **RNN/LSTM/GRU** 作为核心结构，用于捕获时间步之间的依赖关系。
- 输出层采用 **Softmax激活函数**，输出每个时间步的词汇分布。

------

#### **3. 模型训练**

- **输入数据**：将诗歌划分为若干个短句（如每5个字为一个时间步），输入模型。
- **目标数据**：每个时间步的目标是预测下一个字。
- **损失函数**：使用 **交叉熵损失函数**（CrossEntropy Loss）来衡量模型输出与真实目标之间的差异。
- **优化算法**：常用 **Adam**、**SGD** 等优化器来更新模型参数。

------

#### **4. 诗歌生成**

模型训练完成后，生成过程通常包括以下几种策略：

**（1）贪心搜索 (Greedy Search)**

- 每个时间步选择概率最高的字。
- **优点**：生成速度快。
- **缺点**：可能陷入局部最优，缺乏多样性。

**（2）随机采样 (Random Sampling)**

- 在每个时间步根据概率分布随机选择下一个字。
- **优点**：增加生成的多样性。
- **缺点**：可能生成不合理的内容。

**（3）束搜索 (Beam Search)**

- 每个时间步保留多个可能的分支（如前3个概率最高的字），不断扩展并筛选最优路径。
- **优点**：兼顾多样性和合理性，生成质量更高。
- **缺点**：计算量较大。



# 训练截图

![屏幕截图 2025-03-25 155657](C:\Users\ASUS\Pictures\Screenshots\屏幕截图 2025-03-25 155657.png)

![image-20250327133358797](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250327133358797.png)

# 总结

本实验成功实现了一个基于LSTM的古诗生成模型，通过深度学习技术学习了古诗的创作规律。虽然生成的诗歌还存在一些不足，但整体效果已经能够体现出古诗的基本特征。这个实验不仅加深了对RNN/LSTM的理解，也展示了深度学习在自然语言生成领域的应用潜力。对于模型的训练，使用gpu可以明显加速。

